# Learning AI Notes

## æ¢¯åº¦ä¸‹é™

ä¸‹é¢è¿™ä¸ªç¨‹åºä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è®¡ç®—æœ€å°äºŒä¹˜æ³•ï¼š$J(\theta)=\frac{1}{2m}\sum\limits_1^m(h_{\theta}(x)-y)^2$ ï¼Œå…¶ä¸­ $m$ ä¸ºæ•°æ®ç‚¹æ€»æ•°ï¼Œ$y$ ä¸ºæ•°æ®é›†ä¸­æ¯ä¸ªç‚¹çš„çœŸå® $y$ åæ ‡çš„å€¼ï¼Œ$h_{\theta}(x)$ ä¸ºé¢„æµ‹å‡½æ•°ï¼Œ$h_{\theta}(x)=\theta_0+\theta_1x$ ï¼Œå¯»æ‰¾å‡ºæœ€ä½³çš„ $\theta$ ç»„ã€‚

```python
import numpy as np

m = 20

x0 = np.ones((m,1))
x1 = np.arange(1,m+1).reshape(m,1)
x = np.hstack((x0,x1))
y = np.array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m,1)
print(x)
print(y)
alpha = 0.01

def error_function(theta, x, y):
    h_pred = np.dot(x, theta)
    j_theta = (1. / 2 * m) * np.dot(np.transpose(h_pred), h_pred)
    return j_theta

def gradient_function(theta, X, y):
    h_pred = np.dot(X, theta) - y
    return (1. / m) * np.dot(np.transpose(X), h_pred)

def gradient_descent(X, y, alpha):
    theta = np.array([1, 1]).reshape(2, 1)
    gradient = gradient_function(theta, X, y)
    while not np.all(np.absolute(gradient) <= 1e-6):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, y)
    print(np.dot(X, theta))
    return theta

theta = gradient_descent(x, y, alpha)
print('Theta:',theta)
print('Error_function:', error_function(theta, x, y)[0,0])
```

## BP ç¥ç»ç½‘ç»œ

### å‰å‘ä¼ æ’­

éšè—å±‚è¾“å‡ºå€¼å®šä¹‰ï¼š

$$
a_h^{Hl} = W_h^{Hl} \times X_i\\
b_n^{Hl} = f(a_h^{Hl})
$$

å…¶ä¸­ $X_i$ æ˜¯å½“å‰èŠ‚ç‚¹çš„è¾“å…¥å€¼ï¼Œ$W_h^{Hl}$ æ˜¯è¿æ¥åˆ°æ­¤èŠ‚ç‚¹çš„æƒé‡ï¼Œ$a_h^{Hl}$ æ˜¯è¾“å‡ºå€¼ã€‚ $f$ æ˜¯å½“å‰èŠ‚ç‚¹çš„æ¿€æ´»å‡½æ•°ï¼Œ$b_h^{Hl}$ ä¸ºå½“å‰èŠ‚ç‚¹çš„è¾“å…¥å€¼ç»è¿‡è®¡ç®—åè¢«æ¿€æ´»çš„å€¼ã€‚

è¾“å‡ºå±‚å®šä¹‰ï¼š

$$
a_k = \sum (W_{hk}\times b_h^{Hl})
$$

å…¶ä¸­ $W_{hk}$ ä¸ºè¾“å…¥çš„æƒé‡ï¼Œ$b_h^{Hl}$ ä¸ºæœ€åä¸€å±‚éšè—å±‚ç»è¿‡è®¡ç®—åçš„æ¿€æ´»å€¼ï¼Œ$a_k$ ä¸ºç¥ç»ç½‘ç»œçš„æœ€åè¾“å‡ºå€¼ã€‚

### åå‘ä¼ æ’­

å¯¹è¾“å‡ºå±‚ï¼š

$$
\delta_k = \frac{\partial L}{\partial a_k}=(Y-T)
$$

å…¶ä¸­ $\delta_k$ ä¸ºè¾“å‡ºå±‚çš„è¯¯å·®é¡¹ï¼Œå…¶è®¡ç®—å€¼ä¸ºçœŸå®å€¼ä¸æ¨¡å‹è®¡ç®—å€¼ä¹‹é—´çš„å·®å€¼ï¼Œ$Y$ ä¸ºè®¡ç®—å€¼ï¼Œ$T$ ä¸ºçœŸå®å€¼ã€‚

å¯¹éšè—å±‚ï¼š

$$
\delta_h^{Hl}=\frac{\partial L}{\partial a_h^{Hl}}=\frac{\partial L}{\partial b_h^{Hl}}\times\frac{b_h^{Hl}}{a_h^{Hl}}=\frac{\partial L}{\partial b_h^{Hl}}\times f'(a_h^{Hl}) =\frac{\partial L}{\partial a_k}\times\frac{\partial a_k}{\partial b_h^{Hl}}\times f'(a_h^{Hl})\\
=\delta_k\times\sum W_{hk} \times f'(a_h^{Hl})=\sum (W_{hk}\times\delta_k\times f'(a_h^{Hl}))
$$

è¿™é‡Œçš„ $\sum$ æœ‰ä¸€äº›æ¨¡ç³Šï¼Œå®é™…ä¸Šåœ¨ $\frac{\partial a_k}{\partial b_h^{Hl}}$ éƒ¨åˆ†æ˜¯å•ä¸ª $W_{hk}$ï¼Œä½†æ˜¯ç”±äºå¯¹äºæœ€ç»ˆéšè—å±‚çš„ä¸€ä¸ªç‚¹æ¥è¯´ï¼Œè¿™ä¸ªèŠ‚ç‚¹è¾“å‡ºå±‚çš„ç»“æœå‡æœ‰ä¸€å®šæƒé‡çš„è´¡çŒ®ï¼Œå› æ­¤æœ€ååº”å½“ç´¯åŠ ã€‚

### æƒé‡æ›´æ–°

$$
W_{ji}=W_{ji}+a\times\delta_j^l\times x_{ji}\\
b_{ji}=b_{ji}+a\times\delta_j^{l}
$$

æ€»çš„ä»£ç å¦‚ä¸‹ï¼š

```python
# è¿™ä¸ªç¨‹åºæ˜¯ä¸ºäº†ä¸æ–­è®­ç»ƒç¥ç»ç½‘ç»œä½¿å…¶å¯¹äº cases æ•°æ®é¢„æµ‹ç»“æœè¶Šæ¥è¶Šæ¥è¿‘ results
import numpy as np
import math
import random

def rand(a, b):
    return a + (b - a) * random.random()

def make_matrix(m, n, fill = 0.0):
    mat = []
    for i in range(m):
        mat.append([fill] * n)
    return mat

def sigmoid(x):
    return 1.0 / (1.0 + math.exp(-x))

def sigmoid_derivate(x):
    return x * (1 - x)

class BPNeuralNetwork:
    def __init__(self):
        self.input_n = 0
        self.hidden_n = 0
        self.output_n = 0
        self.input_cells = []
        self.hidden_cells = []
        self.output_cells = []

    def setup(self, ni, nh, no):
        self.input_n = ni + 1 # è¿™é‡ŒåŒ…å«äº†è¾“å…¥çš„ bias
        self.hidden_n = nh
        self.output_n = no
        self.input_cells = [1.0] * self.input_n
        self.hidden_cells = [1.0] * self.hidden_n
        self.output_cells = [1.0] * self.output_n
        self.input_weights = make_matrix(self.input_n, self.hidden_n)
        self.output_weights = make_matrix(self.hidden_n, self.output_n)

        for i in range(self.input_n):
            for j in range(self.hidden_n):
                self.input_weights[i][j] = rand(-0.2, 0.2)
        for i in range(self.hidden_n):
            for j in range(self.output_n):
                self.output_weights[i][j] = rand(-2.0, 2.0)
    
    #æ­£å‘è®¡ç®—
    def predict(self,inputs):
        for i in range(self.input_n - 1):
            self.input_cells[i] = inputs[i]
        #ä»è¾“å…¥å±‚è®¡ç®—åˆ°éšè—å±‚
        for i in range(self.hidden_n):
            total = 0.0
            for j in range(self.input_n):
                total += self.input_cells[j] * self.input_weights[j][i]
            self.hidden_cells[i] = sigmoid(total)
        #ä»éšè—å±‚è®¡ç®—åˆ°è¾“å…¥å±‚
        for i in range(self.output_n):
            total = 0.0
            for j in range(self.hidden_n):
                total += self.hidden_cells[j] * self.output_weights[j][i]
            self.output_cells[i] = sigmoid(total)
        return self.output_cells
    
    def back_propagation(self, case, result, learn):
        self.predict(case)
        #è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        output_deltas = [0.0] * self.output_n
        for i in range(self.output_n):
            error = result[i] - self.output_cells[i]
            output_deltas[i] = sigmoid_derivate(self.output_cells[i]) * error
        #è®¡ç®—éšè—å±‚è¯¯å·®(è¿™é‡Œåªæœ‰ä¸€å±‚éšè—å±‚)
        hidden_deltas = [0.0] * self.hidden_n
        for i in range(self.hidden_n):
            error = 0.0
            for j in range(self.output_n):
                error += output_deltas[j] * self.output_weights[i][j]
            hidden_deltas[i] = sigmoid_derivate(self.hidden_cells[i]) * error
        #æ›´æ–°è¾“å‡ºå±‚æƒé‡
        for i in range(self.hidden_n):
            for j in range(self.output_n):
                self.output_weights[i][j] += learn * output_deltas[j] * self.hidden_cells[i]
        #æ›´æ–°éšè—å±‚æƒé‡
        for i in range(self.input_n):
            for j in range(self.hidden_n):
                self.input_weights[i][j] += learn * hidden_deltas[j] * self.input_cells[i]
        error = 0
        for i in range(len(result)):
            error += 0.5 * (result[i] - self.output_cells[i]) ** 2
        return error
    
    def train(self, cases, results, epochs = 1000000, learn = 0.05):
        for epoch in range(epochs):
            error = 0
            for i in range(len(cases)):
                result = results[i]
                case = cases[i]
                error += self.back_propagation(case, result, learn)
    
    def test(self):
        cases = [
            [0, 0],
            [0, 1],
            [1, 0],
            [1, 1]
        ]
        results = [[0], [1], [1], [0]]
        self.setup(2, 5, 1)
        self.train(cases, results)
        for case in cases:
            print(self.predict(case))

if __name__ == '__main__':
    nn = BPNeuralNetwork()
    nn.test()
```

å¯ä»¥çœ‹åˆ°ï¼Œå½“è®­ç»ƒæ¬¡æ•°è¾¾åˆ° 1000000 æ—¶ï¼Œç»“æœå·²ç»éå¸¸æ¥è¿‘ resultsï¼š

![](../../assets/image-20240717102232618.png)

## å·ç§¯ç¥ç»ç½‘ç»œ

### å·ç§¯è¿ç®—

e.g.å¯¹é«˜é€Ÿå…¬è·¯ä¸Šçš„è·‘è½¦çš„ä½ç½®è¿›è¡Œè¿½è¸ªï¼Œå¯¹äºè¿åŠ¨ä¸­çš„ç›®æ ‡ï¼Œé‡‡æ ·æ—¶é—´è¶Šé•¿ï¼Œç”±äºæ»åæ€§çš„åŸå› ï¼Œå®šä½çš„å‡†ç¡®ç‡è¶Šä½ï¼›é‡‡æ ·æ—¶é—´è¶ŠçŸ­ï¼Œå°±è¶Šæ¥è¿‘çœŸå®å€¼ã€‚å› æ­¤ï¼Œå¯ä»¥å¯¹ä¸åŒçš„æ—¶é—´æ®µèµ‹äºˆä¸åŒçš„æƒé‡ï¼Œå³ $s(t)=\int x(a)\omega(t-a)da$ ï¼Œè¿™æ ·çš„è¿ç®—ç§°ä¸ºå·ç§¯è¿ç®—ï¼Œä¹Ÿè®°ä¸º $s(t)=(x\omega)(t)$ã€‚

è€Œå¯¹äºä¸€ä¸ªçŸ©é˜µï¼ˆä¸€èˆ¬å¯ä»¥æ˜¯ä¸€å¼ å›¾ç‰‡ä¿¡æ¯ï¼‰æ¥è¯´ï¼Œæˆ‘ä»¬è®¾å®šä¸€ä¸ªå·ç§¯æ ¸ï¼ˆå…¶å¤§å°ä¸€èˆ¬è¿œå°äºè¾“å…¥çŸ©é˜µï¼‰ï¼Œé€šè¿‡å·ç§¯è¿ç®—åå¯ä»¥æå–å›¾ç‰‡ç‰¹å¾ï¼Œå‡å°‘åç»­çš„è®¡ç®—é‡ã€‚ï¼ˆå¦‚æœå·ç§¯æ ¸è®¾å®šå¾—å½“å¯ä»¥å¯¹å›¾ç‰‡èµ·åˆ°é”åŒ–ã€è·å–è½®å»“ã€æ¨¡ç³Šç­‰æ•ˆæœï¼‰

```python
import numpy as np

dateMat = np.ones((7,7))
kernel = np.array([[2, 1, 1], [3, 0, 1], [1, 1, 0]]) 

def convolve(dateMat, kernel):
    m, n = dateMat.shape
    km, kn = kernel.shape
    newMat = np.ones((m - km + 1, n - kn + 1))
    tempMat = np.ones((km, kn))
    for row in range(m - km + 1):
        for col in range(n - kn + 1):
            for m_k in range(km):
                for n_k in range(kn):
                    tempMat[m_k, n_k] = dateMat[row + m_k, col + n_k] * kernel[m_k, n_k]
            newMat[row, col] = np.sum(tempMat)
    return newMat

print(convolve(dateMat, kernel))
```

### PyTorch çš„å·ç§¯è¿ç®—

PyTorch è®¾å®šäº† API å‡½æ•° Conv2d(Conv) ä½œä¸ºå·ç§¯è®¡ç®—å‡½æ•°ï¼Œå…¶é‡è¦çš„ 5 ä¸ªå‚æ•°å¦‚ä¸‹ï¼š

- in_channelsï¼šè¾“å…¥çš„å·ç§¯æ ¸æ•°ç›®ï¼ˆå¦‚æœå¯¹äºå›¾ç‰‡æ¥è¯´ï¼ŒRGB æ¨¡å¼å°±æœ‰ 3 ä¸ªå·ç§¯æ ¸ï¼Œç°åº¦æ¨¡å¼å°±æœ‰ 1 ä¸ªå·ç§¯æ ¸ï¼‰
- out_channelsï¼šè¾“å‡ºçš„å·ç§¯æ ¸æ•°ç›®
- kernel_sizeï¼šå·ç§¯æ ¸å¤§å°ï¼Œè¾“å…¥ä¸€ä¸ªå‘é‡ï¼Œå…·æœ‰[filter_height,filter_width]çš„ç»´åº¦ï¼Œå¦‚æœä»…ä»…æ˜¯ä¸€ä¸ªæ•° nï¼Œé»˜è®¤ä¸º n*n å¤§å°
- stridesï¼šæ­¥è¿›å¤§å°ï¼Œé»˜è®¤ä¸º 1ï¼›å¦‚æœå‚æ•°æ˜¯ stride=(2,1) é‚£ä¹ˆåœ¨é«˜æ­¥é•¿ä¸º 2ï¼Œåœ¨å®½æ­¥é•¿ä¸º 1
- paddingï¼šè¡¥å…¨æ–¹å¼ï¼Œåªèƒ½ä¸º 0 æˆ– 1ï¼Œpadding ä¸º 1 æ—¶å›¾åƒè¾¹ç¼˜ç”¨ä¸€åœˆ 0 è¡¥é½ï¼Œä½¿å¾—å·ç§¯ç»“æœä¸åŸè¾“å…¥çŸ©é˜µçš„å¤§å°ä¸€è‡´

```python
import torch

image = torch.randn(5, 3, 128, 128)

conv2d = torch.nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1)
image_new_1 = conv2d(image)
print(image_new_1.shape)

conv2d = torch.nn.Conv2d(3, 10, kernel_size=3, stride=2, padding=1)
image_new_2 = conv2d(image)
print(image_new_2.shape)
```

![](../../assets/image-20240717105628030.png)

### æ± åŒ–è¿ç®—

åœ¨å·ç§¯è·å¾—äº†å›¾åƒç‰¹å¾ä¹‹åï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥å¸Œæœ›å¯¹è¿™äº›ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚

ç‰¹å¾æå–å¯ä»¥è®¡ç®—å›¾åƒåœ¨ä¸€ä¸ªåŒºåŸŸä¸Šçš„æŸä¸ªç‰¹å®šç‰¹å¾çš„å¹³å‡å€¼ï¼ˆæˆ–æœ€å¤§å€¼ï¼‰ï¼Œç»“æœä¸ä»…å…·æœ‰ä½å¾—å¤šçš„ç»´åº¦ï¼ŒåŒæ—¶è¿˜ä¼šæ”¹å–„ç»“æœï¼ˆä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ï¼Œè¿™ç§èšåˆçš„æ“ä½œå«åšæ± åŒ–ï¼ˆPoolingï¼‰ã€‚

å¦‚æœé€‰æ‹©å›¾åƒä¸­çš„è¿ç»­èŒƒå›´ä½œä¸ºæ± åŒ–åŒºåŸŸï¼Œå¹¶ä¸”åªæ˜¯æ± åŒ–ç›¸åŒçš„éšè—å•å…ƒäº§ç”Ÿçš„ç‰¹å¾ï¼Œé‚£ä¹ˆè¿™äº›æ± åŒ–å•å…ƒå°±å…·æœ‰å¹³ç§»ä¸å˜æ€§ã€‚è¿™æ„å‘³ç€å³ä½¿å›¾åƒç»å†äº†ä¸€ä¸ªå°çš„å¹³ç§»ï¼Œä¾ç„¶ä¼šäº§ç”Ÿç›¸åŒçš„æ± åŒ–ç‰¹å¾ã€‚æ¯”å¦‚åˆ¤å®šä¸€å¹…å›¾åƒæ˜¯å¦åŒ…å«äººè„¸æ—¶ï¼Œå¹¶ä¸éœ€è¦åˆ¤å®šçœ¼ç›çš„ä½ç½®ï¼Œè€Œæ˜¯éœ€è¦çŸ¥é“æœ‰ä¸€åªçœ¼ç›å‡ºç°åœ¨è„¸éƒ¨çš„å·¦ä¾§ï¼Œå¦ä¸€åªå‡ºç°åœ¨å³ä¾§ã€‚

```python
import torch

image = torch.randn(size = (5, 3, 28, 28))

pool = torch.nn.AvgPool2d(kernel_size = 3, stride = 2, padding = 0)
image_pooled_avg = pool(image)
image_pooled_adaptive_avg = torch.nn.AdaptiveAvgPool2d(1)(image)
print(image_pooled_avg.shape)
print(image_pooled_adaptive_avg.shape)
```

![image-20240717110554082](../../assets/image-20240717110554082.png)

è¿™é‡Œçš„ AdaptiveAvgPool2d å‡½æ•°åˆ™æ˜¯å¯¹æ•´å¼ å›¾ç‰‡è¿›è¡Œæ± åŒ–å¤„ç†ï¼ˆè¿™é‡Œå³ä¸ºå¯¹ 28*28 çš„æ¯ä¸ªå…ƒç´ å–å¹³å‡ï¼‰

### Softmax æ¿€æ´»å‡½æ•°

Softmax æ˜¯ä¸€ä¸ªå¯¹æ¦‚ç‡è¿›è¡Œè®¡ç®—çš„æ¨¡å‹ï¼Œåœ¨çœŸå®çš„è®¡ç®—æ¨¡å‹ç³»ç»Ÿä¸­ï¼Œå¯¹ä¸€ä¸ªå®ç‰©çš„åˆ¤å®šå¹¶é 100% æˆ–è€…æ˜¯ 0%ï¼ŒSoftmax å°±æ˜¯å¯¹æ‰€æœ‰çš„ç»“æœæ ‡ç­¾ä¸Šæ±‚å‡ºä¸€ä¸ªæ¦‚ç‡ã€‚

$$
f(x)=\sum\limits_i^jw_{ij}x_j+b\\
Softmax=\frac{e^{x_i}}{\sum\limits_0^je^{x_j}}\\
y=Softmax(f(x))=Softmax(w_{ij}x_j+b)
$$


### å·ç§¯ç¥ç»ç½‘ç»œåŸç†

å·ç§¯ç¥ç»ç½‘ç»œçš„å¤„ç†ä¸»è¦åŒ…å«ä»¥ä¸‹ 4 ä¸ªæ­¥éª¤ï¼š

- å›¾åƒè¾“å…¥ï¼šè·å–è¾“å…¥çš„æ•°æ®å›¾åƒ

- å·ç§¯ï¼šå¯¹å›¾åƒç‰¹å¾è¿›è¡Œæå–
  - ç»è¿‡å·ç§¯å±‚çš„å›¾åƒè¢«å·ç§¯æ ¸å¿ƒæå–åï¼Œè·å¾—åˆ†å—çš„ã€åŒæ ·å¤§å°çš„å›¾ç‰‡
  - å¯¹åˆ†è§£åçš„å›¾ç‰‡ä½¿ç”¨ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œè¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†ï¼Œå³å°†äºŒä½çŸ©é˜µè½¬åŒ–æˆä¸€ç»´æ•°ç»„
- Pooling å±‚ï¼šç”¨äºç¼©å°åœ¨å·ç§¯æ—¶è·å–çš„å›¾åƒç‰¹å¾
  - å¯¹è·å–çš„å›¾åƒç‰¹å¾è¿›è¡Œç¼©å‡
- å…¨è¿æ¥å±‚ï¼šç”¨äºå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»

```python
import torch
import torch.nn as nn
import numpy as np
import einops.layers.torch as elt

x_train = np.load("x_train.npy")
y_train_label = np.load("y_train_label.npy")

x_train = np.expand_dims(x_train, axis = 1)
# print(x_train.shape)

class MnistNetword(nn.Module):
    def __init__(self):
        super(MnistNetword, self).__init__()
        #å‰ç½®çš„ç‰¹å¾æå–æ¨¡å—
        self.convs_stack = nn.Sequential(
            nn.Conv2d(1, 12, kernel_size=7), #ç¬¬ä¸€ä¸ªå·ç§¯å±‚
            nn.ReLU(),
            nn.Conv2d(12, 24, kernel_size=5),#ç¬¬äºŒä¸ªå·ç§¯å±‚
            nn.ReLU(),
            nn.Conv2d(24, 6, kernel_size=3) #ç¬¬ä¸‰ä¸ªå·ç§¯å±‚
        )
        #æœ€ç»ˆåˆ†ç±»å™¨å±‚
        self.logits_layer = nn.Linear(in_features=1536, out_features=10)

    def forward(self, inputs):
        image = inputs
        x = self.convs_stack(image)

        x = elt.Rearrange('b c h w -> b (c h w)')(x)
        # x = torch.nn.Flatten()(x)
        logits = self.logits_layer(x)
        return logits
    
device = "cuda" if torch.cuda.is_available() else "cpu"
model = MnistNetword().to(device)
# model = torch.compile(model)
loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)

batch_size = 128
for epoch in range(42):
    train_num = len(x_train) // batch_size
    train_loss = 0
    for i in range(train_num):
        start = i * batch_size
        end = start + batch_size

        x_batch = torch.tensor(x_train[start:end]).to(device)
        y_batch = torch.tensor(y_train_label[start:end]).to(device)

        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_loss /= train_num
    accuracy = (pred.argmax(1) == y_batch).type(torch.float32).sum().item() / batch_size
    print("epoch:", epoch, "train_loss:", round(train_loss, 2), "accuracy:", round(accuracy, 2))
```

è¿™é‡Œé‡‡ç”¨ ReLU å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼ŒReLU(x)=max{0,x}

![](../../assets/image-20240717125606885.png)

### æ·±åº¦å¯åˆ†ç¦»å·ç§¯

æ™®é€šå·ç§¯ï¼šä¸€ä¸ªå·ç§¯æ ¸å¤„ç† m ä¸ªè¾“å…¥é€šé“çš„å›¾ç‰‡ä¿¡æ¯ï¼Œå·ç§¯æ ¸ä¸ªæ•°ä¸º nï¼ˆå³è¾“å‡ºé€šé“æ•°ä¸º nï¼‰ï¼Œåˆ™æœ‰ n ä¸ªå¤„ç†ç»“æœï¼ˆæ¯ä¸ªç»“æœé€šé“æ•°ä¸º mï¼‰ï¼Œæœ€å n ä¸ªå·ç§¯æ ¸çš„ç»“æœç»„æˆæœ€ç»ˆç»“æœã€‚

![è¯·æ·»åŠ å›¾ç‰‡æè¿°](../../assets/e7145875df9748c594b66983b2c83b95.jpeg)

æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼šåˆ†ä¸ºæ·±åº¦å·ç§¯+é€ç‚¹å·ç§¯

- æ·±åº¦å·ç§¯ï¼šä¸€ä¸ªå·ç§¯æ ¸å¤„ç†ä¸€ä¸ªé€šé“çš„å›¾ç‰‡ä¿¡æ¯ï¼Œm ä¸ªè¾“å…¥é€šé“æœ‰ m ä¸ªå·ç§¯æ ¸ï¼Œm ä¸ªå¤„ç†ç»“æœã€‚

![è¯·æ·»åŠ å›¾ç‰‡æè¿°](../../assets/3f09a176a66548f2a16c207c593302a7.jpeg)

- é€ç‚¹å·ç§¯ï¼šåšå·ç§¯æ ¸å¤§å°ä¸º 1 çš„æ™®é€šå·ç§¯ã€‚

<img src=../../assets/417d4b609b624021a1e631582e2b1451.jpeg alt="è¯·æ·»åŠ å›¾ç‰‡æè¿°" style="zoom: 50%;" />

æ·±åº¦å¯åˆ†ç¦»å·ç§¯å¯ä»¥æœ‰æ•ˆå‡å°‘å¾…è®­ç»ƒå‚æ•°é‡ã€‚

### è†¨èƒ€å·ç§¯

è†¨èƒ€å·ç§¯åˆç§°ç©ºæ´å·ç§¯ï¼Œåœ¨å·ç§¯æ ¸ä¸­å¢åŠ ç©ºæ´ï¼Œå¯ä»¥å¢åŠ å•ä½é¢ç§¯ä¸­è®¡ç®—çš„å¤§å°ï¼Œä»è€Œæ‰©å¤§æ¨¡å‹çš„è®¡ç®—è§†é‡ã€‚

![img](https://pic2.zhimg.com/80/v2-f58615821f14fcb6585b2fc7fa1a2a11_1440w.webp)

æ™®é€šå·ç§¯å³ä¸ºè†¨èƒ€å·ç§¯ dilation=1 çš„æƒ…å†µã€‚

```python
import torch
import torch.nn as nn
import numpy as np
import einops.layers.torch as elt

x_train = np.load('x_train.npy')
y_train_label = np.load('y_train_label.npy')
x_train = np.expand_dims(x_train, axis = 1)

# è‡ªå®šä¹‰æ·±åº¦ã€å¯åˆ†ç¦»ã€è†¨èƒ€å·ç§¯
depth_conv = nn.Conv2d(in_channels = 12, out_channels = 12, kernel_size = 3, groups = 6, dilation = 2)
point_conv = nn.Conv2d(in_channels = 12, out_channels = 24, kernel_size = 1)
depth_separable_conv = torch.nn.Sequential(depth_conv, point_conv)

class MnistNetword(nn.Module):
    def __init__(self):
        super(MnistNetword, self).__init__()
        self.convs_stack = nn.Sequential(
            nn.Conv2d(1, 12, kernel_size = 7),
            nn.ReLU(),
            depth_separable_conv, #ä½¿ç”¨è‡ªå®šä¹‰å·ç§¯æ›¿ä»£åŸç”Ÿå·ç§¯å±‚
            nn.ReLU(),
            nn.Conv2d(24, 6, kernel_size = 3)
        )

        self.logits_layer = nn.Linear(in_features = 1536, out_features = 10)

    def forward(self, inputs):
        image = inputs
        x = self.convs_stack(image)
        x = elt.Rearrange('b c h w -> b (c h w)')(x)
        logits = self.logits_layer(x)
        return logits
    
device = "cuda" if torch.cuda.is_available() else "cpu"
model = MnistNetword().to(device)
# model = torch.compile(model)
loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)

batch_size = 128
for epoch in range(63):
    train_num = len(x_train) // batch_size
    train_loss = 0
    for i in range(train_num):
        start = i * batch_size
        end = start + batch_size

        x_batch = torch.tensor(x_train[start:end]).to(device)
        y_batch = torch.tensor(y_train_label[start:end]).to(device)

        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    train_loss /= train_num
    accuracy = (pred.argmax(1) == y_batch).type(torch.float32).sum().item() / batch_size
    print("epoch:", epoch, "train_loss:", round(train_loss, 2), "accuracy:", round(accuracy, 2))
```

<img src="../../assets/image-20240717144817837.png" alt="image-20240717144817837" style="zoom:50%;" />

## PyTorch æ•°æ®å¤„ç†ä¸æ¨¡å‹å±•ç¤º

### torch.utils.data Dataset å°è£…è‡ªå®šä¹‰æ•°æ®é›†

åœ¨ PyTorch 2.0 ä¸­ï¼Œæ•°æ®é›†çš„è‡ªå®šä¹‰ä½¿ç”¨éœ€è¦ç»§æ‰¿ __torch.utils.data.Dataset__ ç±»ï¼ŒåŸºæœ¬çš„ Dataset ç±»æ¶æ„å¦‚ä¸‹ï¼š

```python
class Dataset():
	def __init__(self, transform = None):
		super(Dataset, self).__init__()
		
	def __getitem__(self, index):
		pass
	
	def __len__(self):
		pass
```

Dataset é™¤äº†åŸºæœ¬çš„ init å‡½æ•°ï¼Œè¿˜éœ€è¦è¡¥å……é¢å¤–çš„ä¸¤ä¸ªå‡½æ•°ï¼š__getitem__ å’Œ __len__ï¼Œè¿™æ˜¯ä»¿ç…§æ•°ç»„è¿›è¡Œçš„å®šä¹‰ï¼Œè¿”å› index å¯¹åº”çš„æ•°æ®ä»¥åŠèŒƒå›´æ•°æ®é›†å¤§å°ã€‚

### æ”¹å˜æ•°æ®ç±»å‹çš„ Dataset ç±»ä¸­çš„ transform

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è‡ªå®šä¹‰çš„æ•°æ®å¤„ç†è½¬æ¢ç±»æ¥å¯¹è¾“å…¥æ•°æ®è¿›è¡Œä¸€äº›ç±»å‹ä¸Šçš„å¤„ç†ã€‚

```python
import numpy as np
import torch

class ToTensor1:
    def __call__(self, inputs, targets):
        return torch.tensor(inputs), torch.tensor(targets)
    
class ToTensor2:
    def __call__(self, inputs, targets):
        inputs = np.reshape(inputs, [28 * 28])
        return torch.tensor(inputs), torch.tensor(targets)
    
class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self, transform = None): # åœ¨å®šä¹‰æ—¶éœ€è¦æ˜ç¡®ç»™å‡ºè¿™é‡Œå®šä¹‰äº†transform
        super(MNIST_Dataset, self).__init__()

        self.x_train = np.load('x_train.npy')
        self.y_train_label = np.load('y_train_label.npy')

        self.transform = transform
        
    def __len__(self):
        return len(self.y_train_label)
    
    def __getitem__(self, index):
        image = self.x_train[index]
        label = self.y_train_label[index]

        if self.transform:
            image, label = self.transform(image, label)
        return image, label

mnist_dataset = MNIST_Dataset()
image, label = mnist_dataset[1024]
print(type(image), type(label))
print(image.shape)
print("--------------------------")
mnist_dataset = MNIST_Dataset(transform = ToTensor1())
image, label = mnist_dataset[1024]
print(type(image), type(label))
mnist_dataset = MNIST_Dataset(transform = ToTensor2())
print(image.shape)
```

<img src="../../assets/image-20240717152439180.png" alt="image-20240717152439180" style="zoom:50%;" />

### æ‰¹é‡è¾“å‡ºæ•°æ®çš„ DataLoader ç±»

DataLoader å¯ä»¥è§£å†³ä½¿ç”¨ Dataset è‡ªå®šä¹‰å°è£…çš„æ•°æ®æ—¶æ— æ³•å¯¹æ•°æ®è¿›è¡Œæ‰¹é‡åŒ–å¤„ç†çš„é—®é¢˜ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š

```python
mnist_dataset = MNIST_Dataset(transform = ToTenSor())
from torch.utils.data import DataLoader
train_loader = DataLoader(mnist_dataset, batch_size = batch_size), shuffle = True)
```

## ResNet

ä¼ ç»Ÿ CNN åœ¨ ResNet å‘å¸ƒä¹‹å‰éƒ½æ˜¯å·ç§¯å±‚ä¸æ± åŒ–å±‚çš„ä¸æ–­å åŠ ï¼Œäººä»¬è®¤ä¸ºå·ç§¯å±‚ä¸æ± åŒ–å±‚çš„å åŠ å±‚æ•°è¶Šå¤šï¼Œè·å¾—å›¾ç‰‡ç‰¹å¾ä¹Ÿå°±è¶Šå…¨ï¼Œå­¦ä¹ æ•ˆæœä¹Ÿå°±è¶Šå¥½ï¼Œä½†äº‹å®ä¸Šç½‘ç»œè¿‡æ·±ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€æ¢¯åº¦çˆ†ç‚¸ã€é€€åŒ–é—®é¢˜ã€‚

ä¸ºäº†è§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒResNet é‡‡ç”¨äº† BatchNormalization çš„å¤„ç†æ–¹æ³•ï¼Œæ‰¹é‡æ­£åˆ™åŒ–å¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚

ä¸ºäº†è§£å†³é€€åŒ–é—®é¢˜ï¼ŒResNet æå‡ºäº†æ®‹å·®ç»“æ„ï¼Œä½¿ç”¨ä¸€ç§ shortcut çš„è¿æ¥æ–¹å¼ï¼Œè®©ç‰¹å¾çŸ©é˜µéš”å±‚ç›¸åŠ ï¼Œ

<img src="https://pic4.zhimg.com/80/v2-1ae69df03709e46bc3cc6d9e2d35930b_1440w.webp" alt="img" style="zoom:50%;" />

å·¦å›¾æ˜¯åŸºäºæ™®é€šå·ç§¯çš„äºŒå±‚æ®‹å·®å­¦ä¹ å•å…ƒï¼Œç§°ä¸º BasicBlockï¼›å³å›¾ç¬¬ä¸€å±‚é‡‡ç”¨ $1\times1$ å·ç§¯æ ¸ï¼Œå¯¹ç‰¹å¾çŸ©é˜µè¿›è¡Œé™ç»´æ“ä½œï¼Œå°†ç‰¹å¾çŸ©é˜µçš„æ·±åº¦ç”± 256 é™ä¸º 64ï¼›ç¬¬ä¸‰å±‚ $1\times1$ å·ç§¯æ ¸å¯¹ç‰¹å¾çŸ©é˜µè¿›è¡Œå‡ç»´æ“ä½œï¼Œå°†ç‰¹å¾çŸ©é˜µçš„æ·±åº¦ç”± 64 å‡ä¸º 256ï¼Œæ•´ä½“ç§°ä¸º Bottleneckï¼Œèƒ½æ›´å¥½å‡å°‘å‚æ•°ã€‚

å¯¹äºç»å…¸ BasicBlock æ¨¡å‹çš„ä»£ç ï¼š

```python
import torch
import torch.nn as nn

class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, in_channels, out_channels, stride = 1):
        super().__init__()
        
        #residual function
        self.residual_function = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace = True),
            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size = 3, padding = 1, bias = False),
            nn.BatchNorm2d(out_channels * BasicBlock.expansion)
        )
        
        #shortcut
        self.shortcut = nn.Sequential()
        #åˆ¤å®šè¾“å‡ºçš„ç»´åº¦æ˜¯å¦å’Œè¾“å…¥ç›¸ä¸€è‡´
        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size = 1, stride = stride, bias = False),
                nn.BatchNorm2d(out_channels * BasicBlock.expansion)
            )
       
    def forward(self, x):
        return nn.ReLU(inplace = True)(self.residual_function(x) + self.shortcut(x))
```

ResNet ç»“æ„ï¼š

![img](https://pic2.zhimg.com/80/v2-46ea2c7eadbf45cf299249ce7f56925d_1440w.webp)

å¯¹äºæ‰€æœ‰çš„ ResNet ç»“æ„ï¼Œconv3_xï¼Œconv4_xï¼Œconv5_x çš„ç¬¬ä¸€å±‚å‡ä¸ºè™šçº¿æ®‹å·®ç»“æ„ï¼Œå…¶ä½™å‡ä¸ºå®çº¿æ®‹å·®ç»“æ„ï¼Œä¸¤è€…åŒºåˆ«å¦‚ä¸‹ï¼šï¼ˆå…¶å®è™šçº¿ä¸å®çº¿çš„å·®åˆ«å°±åœ¨äºè™šçº¿è¿›è¡Œäº†è§„æ¨¡çš„è½¬æ¢ï¼Œä½¿å¾—çŸ©é˜µåŠ æ³•èƒ½é¡ºåˆ©è¿›è¡Œï¼‰

æµ…å±‚ï¼ˆ18/34ï¼‰ï¼š

<img src="https://pic1.zhimg.com/80/v2-b87b43af5d42670e2b1c15b8fc5d6780_1440w.webp" alt="img" style="zoom:50%;" />

æ·±å±‚ï¼ˆ50/101/152ï¼‰ï¼š

<img src="https://pic1.zhimg.com/80/v2-b3c148ca6aee1c5fe8a46488c744d3e4_1440w.webp" alt="img" style="zoom:50%;" />

```python
import pickle
import numpy as np
import os
import torch
import get_data
from resnet import resnet18

def get_cifar10_train_data_and_label(root = ""):
    def load_file(filename):
        with open(filename, 'rb') as fo:
            data = pickle.load(fo, encoding='latin1')
        return data
    
    data_batch_1 = load_file(os.path.join(root, "data_batch_1"))
    data_batch_2 = load_file(os.path.join(root, "data_batch_2"))
    data_batch_3 = load_file(os.path.join(root, "data_batch_3"))
    data_batch_4 = load_file(os.path.join(root, "data_batch_4"))
    data_batch_5 = load_file(os.path.join(root, "data_batch_5"))
    dataset = []
    labelset = []
    for data in [data_batch_1, data_batch_2, data_batch_3, data_batch_4, data_batch_5]:
        dataset.append(data['data'])
        labelset.append(data['labels'])
    dataset = np.concatenate(dataset) # æ‹¼æ¥ç»Ÿä¸€ dataset æ ¼å¼
    labelset = np.concatenate(labelset)
    return dataset, labelset

def get_cifar10_test_data_and_label(root = ""):
    def load_file(filename):
        with open(filename, 'rb') as fo:
            data = pickle.load(fo, encoding='latin1')
        return data
    data_batch_1 = load_file(os.path.join(root, "test_batch"))
    dataset = []
    labelset = []
    for data in [data_batch_1]:
        dataset.append(data['data'])
        labelset.append(data['labels'])
    dataset = np.concatenate(dataset)
    labelset = np.concatenate(labelset)
    return dataset, labelset

def get_CIFAR10_dataset(root = ""):
    train_dataset, label_dataset = get_cifar10_train_data_and_label(root = root)
    test_dataset, test_label_dataset = get_cifar10_test_data_and_label(root = root)
    return train_dataset, label_dataset, test_dataset, test_label_dataset

if __name__ == "__main__":
    train_dataset, label_dataset, test_dataset, test_label_dataset = get_data.get_CIFAR10_dataset(root = "cifar-10-batches-py")
    
    train_dataset = np.reshape(train_dataset, [len(train_dataset), 3, 32, 32]).astype(np.float32) / 255
    test_dataset = np.reshape(test_dataset, [len(test_dataset), 3, 32, 32]).astype(np.float32) / 255
    label_dataset = np.array(label_dataset)
    test_label_dataset = np.array(test_label_dataset)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = resnet18()
    model = model.to(device)
    # model = torch.compile(model)
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
    loss_fn = torch.nn.CrossEntropyLoss()

    batch_size = 128
    train_num = len(train_dataset) // batch_size
    for epoch in range(63):
        train_loss = 0
        for i in range(train_num):
            start = i * batch_size
            end = (i + 1) * batch_size

            x_batch = torch.from_numpy(train_dataset[start:end]).to(device)
            y_batch = torch.from_numpy(label_dataset[start:end]).to(device)

            pred = model(x_batch)
            loss = loss_fn(pred, y_batch.long())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
        
        train_loss /= train_num
        accuracy = (pred.argmax(1) == y_batch).type(torch.float32).sum().item() / batch_size

        test_num = 2048
        x_test = torch.from_numpy(test_dataset[:test_num]).to(device)
        y_test = torch.from_numpy(test_label_dataset[:test_num]).to(device)
        pred = model(x_test)
        test_accuracy = (pred.argmax(1) == y_test).type(torch.float32).sum().item() / test_num
        print("epoch: ", epoch, "train_loss: ", round(train_loss, 2), "accuracy: ", round(accuracy, 2), "test_accuracy: ", round(test_accuracy, 2))
```

## è¯åµŒå…¥

è¯åµŒå…¥æœ¬è´¨ä¸Šæ˜¯ä¸€ç±»æŠ€æœ¯ï¼Œå•ä¸ªè¯åœ¨é¢„å®šä¹‰çš„å‘é‡ç©ºé—´ä¸­è¢«è¡¨ç¤ºä¸ºå®æ•°å‘é‡ï¼Œæ¯ä¸ªå•è¯éƒ½æ˜ å°„åˆ°ä¸€ä¸ªå‘é‡ã€‚

### æ–‡æœ¬æ¸…æ´—

å¯¹äºä¸€ç¯‡æ–°é—»ï¼Œå†…å«æ ‡ç‚¹ç¬¦å·å’Œä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å¯¹æ•´ç¯‡æ–°é—»è¿›è¡Œæ¸…æ´—ï¼Œè·å¾—è¾ƒä¸ºå¹²å‡€çš„è¯åº“ã€‚åŒæ—¶æ–‡ç« é‡Œé¢ä¼šå«å¾ˆå¤šæ²¡æœ‰æ„ä¹‰çš„å‰¯è¯ï¼ˆå¦‚ is,are,the ç­‰ï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ©ç”¨æ¶ˆé™¤åœç”¨è¯ï¼ˆpython nltk å·¥å…·åŒ…ï¼‰å‡å°‘è¦å¤„ç†çš„è¯æ±‡é‡ä»¥å‡å°‘åç»­ç¨‹åºçš„å¤æ‚åº¦ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè‹±æ–‡å•è¯ç”±äºæ—¶æ€ã€å¤æ•°ç­‰å˜åŒ–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ nltk ä¸­çš„ PorterStemmer å·¥å…·å°†å…¶è¿˜åŸæˆåŸè¯å¹²ã€‚

```python
import csv
import numpy as np
import re
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

def text_clear(text):
    text = text.lower() # è½¬æ¢ä¸ºå°å†™
    text = re.sub(r'[^a-z0-9]', ' ', text) # æ›¿æ¢éå­—æ¯æ•°å­—å­—ç¬¦ä¸ºç©ºæ ¼
    text = re.sub(r' +', ' ', text) # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼
    text = text.strip() # å»æ‰é¦–å°¾ç©ºæ ¼
    text = text.split(' ') # åˆ†è¯
    text = [word for word in text if word not in stoplist] # å»é™¤åœç”¨è¯
    text = [PorterStemmer().stem(word) for word in text] # è¿˜åŸè¯å¹²
    text.append("eos") # æ·»åŠ ç»“æŸç¬¦
    text = ["bos"] + text # æ·»åŠ å¼€å§‹ç¬¦
    return text

agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('train.csv', 'r'))
stoplist = stopwords.words('english')
for line in agnews_train:
    agnews_label.append(np.float32(line[0]))
    agnews_title.append(text_clear(line[1]))
    agnews_text.append(text_clear(line[2]))
print(agnews_label)
print(agnews_title)
print(agnews_text)
```

### è¯å‘é‡è®­ç»ƒæ¨¡å‹ Word2Vec

Word2Vec æ˜¯è½»é‡çº§çš„ç¥ç»ç½‘ç»œï¼Œå…¶æ¨¡å‹ä»…ä»…åŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ï¼Œæ¨¡å‹æ¡†æ¶æ ¹æ®è¾“å…¥è¾“å‡ºçš„ä¸åŒã€‚Word2Vec åŒ…æ‹¬ä¸¤ç§æ¨¡å‹ï¼šä¸»è¦åŒ…æ‹¬ CBOW å’Œ Skip-gram æ¨¡å‹ã€‚ CBOW çš„æ–¹å¼æ˜¯åœ¨çŸ¥é“è¯ $w_t$ çš„ä¸Šä¸‹æ–‡$ğ‘¤_{ğ‘¡âˆ’2},ğ‘¤_{ğ‘¡âˆ’1},ğ‘¤_{ğ‘¡+1},ğ‘¤_{ğ‘¡+2}$ çš„æƒ…å†µä¸‹é¢„æµ‹å½“å‰è¯ $ğ‘¤_ğ‘¡$ï¼Œè€Œ Skip-gram æ˜¯åœ¨çŸ¥é“äº†è¯ $ğ‘¤_ğ‘¡$ çš„æƒ…å†µä¸‹ï¼Œå¯¹è¯ $ğ‘¤_ğ‘¡$ çš„ä¸Šä¸‹æ–‡ $ğ‘¤_{ğ‘¡âˆ’2},ğ‘¤_{ğ‘¡âˆ’1},ğ‘¤_{ğ‘¡+1},ğ‘¤_{ğ‘¡+2}$ è¿›è¡Œé¢„æµ‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![img](https://pic3.zhimg.com/80/v2-ded34fa32e7e10eb5c5f24b6c4048d3e_1440w.webp)

CBOW æ¨¡å‹æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡å»é¢„æµ‹ç›®æ ‡è¯æ¥è®­ç»ƒå¾—åˆ°è¯å‘é‡ï¼Œè€Œ Skip-gram æ¨¡å‹åˆ™æ˜¯æ ¹æ®ç›®æ ‡è¯å»é¢„æµ‹ä¸Šä¸‹æ–‡æ¥è®­ç»ƒå¾—åˆ°è¯å‘é‡ã€‚CBOW é€‚åˆäºæ•°æ®é›†è¾ƒå°çš„æƒ…å†µï¼Œè€Œ Skip-Gram åœ¨å¤§å‹è¯­æ–™ä¸­è¡¨ç°æ›´å¥½ã€‚

Python å®ç° Word2Vec æ¨¡å‹ï¼Œéœ€è¦ `from gensim.models import word2vec` å¯¼å…¥ word2vec å‡½æ•°ï¼Œå…¶ä¸»è¦å‚æ•°ï¼š`Word2Vec(sentences, workers = num_workers, vector_size = num_features, min_count = min_word_count, window = context, sample = downsampling, epochs = 5)` å…¶ä¸­ sentences æ˜¯è¾“å…¥æ•°æ®ï¼Œworker æ˜¯å¹¶è¡Œè¿è¡Œçš„çº¿ç¨‹æ•°ï¼Œvector_size æ˜¯è¯å‘é‡çš„ç»´æ•°ï¼Œmin_count æ˜¯æœ€å°çš„è¯é¢‘ï¼Œwindowæ˜¯ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œsample æ˜¯å¯¹é¢‘ç¹è¯æ±‡ä¸‹é‡‡æ ·è®¾ç½®ï¼Œepochs æ˜¯å¾ªç¯çš„æ¬¡æ•°ï¼Œä¸€èˆ¬ä¸æ˜¯æœ‰ç‰¹æ®Šè¦æ±‚ï¼ŒæŒ‰é»˜è®¤å€¼è®¾ç½®å³å¯ã€‚

```python
import csv
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def text_clear(text):
    text = text.lower() # è½¬æ¢ä¸ºå°å†™
    text = re.sub(r'[^a-z0-9]', ' ', text) # æ›¿æ¢éå­—æ¯æ•°å­—å­—ç¬¦ä¸ºç©ºæ ¼
    text = re.sub(r' +', ' ', text) # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼
    text = text.strip() # å»æ‰é¦–å°¾ç©ºæ ¼
    text = text.split(' ') # åˆ†è¯
    text = [word for word in text if word not in stoplist] # å»é™¤åœç”¨è¯
    text = [PorterStemmer().stem(word) for word in text] # è¿˜åŸè¯å¹²
    text.append("eos") # æ·»åŠ ç»“æŸç¬¦
    text = ["bos"] + text # æ·»åŠ å¼€å§‹ç¬¦
    return text

agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('train.csv', 'r'))
stoplist = stopwords.words('english')
for line in agnews_train:
    agnews_label.append(np.float32(line[0]))
    agnews_title.append(text_clear(line[1]))
    agnews_text.append(text_clear(line[2]))

print("å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š")
from gensim.models import word2vec
model = word2vec.Word2Vec(agnews_text, vector_size = 64, min_count = 0, window = 5, epochs = 128)
model_name = "corpusWord2Vec.bin"
model.save(model_name)
from gensim.models import word2vec
model = word2vec.Word2Vec.load('./corpusWord2Vec.bin') #ä»¥ä¸Šæ¬¡è®­ç»ƒçš„æ•°æ®ä¸ºåŸºç¡€ç»§ç»­è®­ç»ƒ
model.train(agnews_title, epochs = model.epochs, total_examples = model.corpus_count)
```

### æ–‡æœ¬ä¸»é¢˜æå–ï¼šåŸºäº TF-IDF

TF-IDF æ˜¯ä¸€ç§ç”¨äºèµ„è®¯æ£€ç´¢ä¸å’¨è¯¢å‹˜æµ‹çš„å¸¸ç”¨åŠ æƒæŠ€æœ¯ã€‚ TF-IDF æ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨æ¥è¡¡é‡ä¸€ä¸ªè¯å¯¹ä¸€ä¸ªæ–‡ä»¶é›†çš„é‡è¦ç¨‹åº¦ã€‚å­—è¯çš„é‡è¦æ€§ä¸å…¶åœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”ï¼Œè€Œä¸å…¶åœ¨æ–‡ä»¶é›†ä¸­å‡ºç°çš„æ¬¡æ•°æˆåæ¯”ã€‚

$$
è¯é¢‘(TF)=\frac{æŸä¸ªè¯åœ¨å•ä¸ªæ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°}{æŸä¸ªè¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­å‡ºç°çš„æ¬¡æ•°}\\
é€†æ–‡æ¡£æ¦‚ç‡(IDF)=\log(\frac{è¯­æ–™åº“çš„æ–‡æœ¬æ€»æ•°}{è¯­æ–™åº“ä¸­åŒ…å«è¯¥è¯çš„æ–‡æœ¬æ•°+1})\\
TF-IDF=TF\times IDF
$$


```python
import csv
import numpy as np
import re
import math
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

class TFIDF_score:
    def __init__(self, corpus, model = None):
        self.corpus = corpus
        self.model = model
        self.idfs = self.__idf()

    def __idf(self):
        idfs = {}
        d = 0.0
        for doc in self.corpus:
            d += 1
            counted = []
            for word in doc:
                if word not in counted:
                    if word in idfs:
                        idfs[word] += 1
                    else:
                        idfs[word] = 1
                    counted.append(word)
        for word in idfs:
            idfs[word] = math.log(d / float(idfs[word]))
        return idfs
    def __get_TFIDF_score(self, text):
        word_tfidf = {}
        for word in text:
            if word in word_tfidf:
                word_tfidf[word] += 1
            else:
                word_tfidf[word] = 1
        for word in word_tfidf:
            word_tfidf[word] *= self.idfs[word]
        values_list = sorted(word_tfidf.items(), key=lambda item: item[1], reverse=True)
        return values_list
    
    def get_TFIDF_result(self, text):
        values_list = self.__get_TFIDF_score(text)
        value_list = []
        for value in values_list:
            value_list.append(value[0])
        return value_list

def text_clear(text):
    text = text.lower() # è½¬æ¢ä¸ºå°å†™
    text = re.sub(r'[^a-z0-9]', ' ', text) # æ›¿æ¢éå­—æ¯æ•°å­—å­—ç¬¦ä¸ºç©ºæ ¼
    text = re.sub(r' +', ' ', text) # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼
    text = text.strip() # å»æ‰é¦–å°¾ç©ºæ ¼
    text = text.split(' ') # åˆ†è¯
    text = [word for word in text if word not in stoplist] # å»é™¤åœç”¨è¯
    text = [PorterStemmer().stem(word) for word in text] # è¿˜åŸè¯å¹²
    text.append("eos") # æ·»åŠ ç»“æŸç¬¦
    text = ["bos"] + text # æ·»åŠ å¼€å§‹ç¬¦
    return text

agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('train.csv', 'r'))
stoplist = stopwords.words('english')
for line in agnews_train:
    agnews_label.append(np.float32(line[0]))
    agnews_title.append(text_clear(line[1]))
    agnews_text.append(text_clear(line[2]))

tfidf = TFIDF_score(agnews_text)
for line in agnews_text:
    value_list = tfidf.get_TFIDF_result(line)
    print(value_list)
```

### *æ–‡æœ¬ä¸»é¢˜çš„æå–ï¼šåŸºäº TextRank*

TextRank ç®—æ³•æ ¸å¿ƒæ€æƒ³æ¥æºäºç½‘é¡µæ’åç®—æ³• PageRankï¼ŒPageRank çš„æ€æƒ³ï¼š

- å½“ä¸€ä¸ªç½‘é¡µè¢«è¶Šå¤šç½‘é¡µæ‰€é“¾æ¥æ—¶ï¼Œå…¶æ’åä¼šè¶Šé å‰ã€‚
- æ’åé«˜çš„ç½‘é¡µåº”å…·æœ‰æ›´å¤§çš„è¡¨å†³æƒï¼Œå³å½“ä¸€ä¸ªç½‘é¡µè¢«æ’åé«˜çš„ç½‘é¡µæ‰€é“¾æ¥æ—¶ï¼Œå…¶é‡è¦æ€§ä¹Ÿä¼šå¯¹åº”æé«˜ã€‚

TextRank æ­¥éª¤å¦‚ä¸‹ï¼š

> (1) æŠŠç»™å®šçš„æ–‡æœ¬ T æŒ‰ç…§å®Œæ•´å¥å­è¿›è¡Œåˆ†å‰²ã€‚
>
> (2) å¯¹äºæ¯ä¸ªå¥å­ï¼Œè¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨å¤„ç†ï¼Œå¹¶è¿‡æ»¤æ‰åœç”¨è¯ï¼Œåªä¿ç•™æŒ‡å®šè¯æ€§çš„å•è¯ï¼Œå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ã€‚
>
> (3) æ„å»ºå€™é€‰å…³é”®è¯å›¾ G=(V,E)ï¼Œå…¶ä¸­ V ä¸ºèŠ‚ç‚¹é›†ï¼Œç”±æ¯ä¸ªè¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ä½œä¸ºè¿æ¥çš„è¾¹å€¼ã€‚
>
> (4) æ ¹æ®å…¬å¼ï¼š
> $$
> WS(V_i)=(1-d)+d\times \sum\limits_{V_j\in In(V_i)}\frac{w_{ji}}{\sum\limits_{V_k\in Out(V_j)}w_{jk}}WS(V_j)
> $$
> è¿­ä»£ä¼ æ’­å„èŠ‚ç‚¹çš„æƒé‡ï¼Œç›´è‡³æ”¶æ•›ã€‚æœ€åå¯¹èŠ‚ç‚¹æƒé‡è¿›è¡Œå€’åºæ’åºï¼Œä½œä¸ºæŒ‰é‡è¦ç¨‹åº¦æ’åˆ—çš„å…³é”®è¯ã€‚

### FastText å’Œé¢„è®­ç»ƒè¯å‘é‡

#### FastText åŸç†ä¸åŸºç¡€ç®—æ³•

FastText é‡‡ç”¨ä¸¤ä¸ªé‡è¦çš„ç®—æ³•ï¼šN-Gram å’Œ Hierarchical Softmaxã€‚

##### N-Gran æ¶æ„

<img src="https://i-blog.csdnimg.cn/blog_migrate/f4bf1ad0bc489e78e8334bacecfbf247.png" alt="img" style="zoom:50%;" />

å…¶ä¸­ $x_1,x_2,...,x_{N-1},x_N$ è¡¨ç¤ºä¸€ä¸ªæ–‡æœ¬ä¸­çš„ N-Gram å‘é‡ï¼Œæ¯ä¸ªç‰¹å¾æ˜¯è¯å‘é‡çš„å¹³å‡å€¼ã€‚

å¸¸ç”¨çš„ N-Gram æ¶æ„æœ‰ä¸‰ç§ï¼š1-Gramï¼Œ2-Gram å’Œ 3-Gramã€‚

**e.g.â€œæˆ‘æƒ³å»æˆéƒ½åƒç«é”…â€**

1-Gramï¼š[â€œæˆ‘â€ï¼Œâ€œæƒ³â€ï¼Œâ€œå»â€ï¼Œâ€œæˆâ€ï¼Œâ€œéƒ½â€ï¼Œâ€œåƒâ€ï¼Œâ€œç«â€ï¼Œâ€œé”…â€]

2-Gramï¼š[â€œæˆ‘æƒ³â€ï¼Œâ€œæƒ³å»â€ï¼Œâ€œå»æˆâ€ï¼Œâ€œæˆéƒ½â€ï¼Œâ€œéƒ½åƒâ€ï¼Œâ€œåƒç«â€ï¼Œâ€œç«é”…â€]

3-Gramï¼š[â€œæˆ‘æƒ³å»â€ï¼Œâ€œæƒ³å»æˆâ€ï¼Œâ€œå»æˆéƒ½â€ï¼Œâ€œæˆéƒ½åƒâ€ï¼Œâ€œéƒ½åƒç«â€ï¼Œâ€œåƒç«é”…â€]

##### Hierarchical Softmax æ¶æ„

FastText ä¸­çš„ Hierarchical Softmax åˆ©ç”¨ Huffmanæ ‘å®ç°ï¼Œå°†è¯å‘é‡ä½œä¸ºå¶å­èŠ‚ç‚¹ã€‚

#### FastText è®­ç»ƒåŠå…¶ä¸ PyTorch 2.0 çš„ååŒä½¿ç”¨

gensim.model ä¸­é™¤äº†åŒ…å« Word2Vec å‡½æ•°å¤–ï¼Œè¿˜åŒ…å« FastText çš„ä¸“ç”¨è®¡ç®—ç±»ï¼Œå…¶å‚æ•°å®šä¹‰å¦‚ä¸‹ï¼š

- `sentences(iterable of iterables, optional)`ï¼šä¾›è®­ç»ƒçš„å¥å­ï¼Œå¯ä»¥ä½¿ç”¨ç®€å•çš„åˆ—è¡¨ï¼Œä½†æ˜¯å¯¹äºå¤§è¯­æ–™åº“ï¼Œå»ºè®®ç›´æ¥ä»ç£ç›˜/ç½‘ç»œæµè¿­ä»£ä¼ è¾“å¥å­ã€‚
- `vector_size(int, optional)` ï¼šè¯å‘é‡çš„ç»´åº¦ã€‚
- `window(int, optional)` ï¼šä¸€ä¸ªå¥å­ä¸­å½“å‰å•è¯å’Œè¢«é¢„æµ‹å•è¯çš„æœ€å¤§è·ç¦»ã€‚
- `min_count(int, optional)` ï¼šå¿½ç•¥è¯é¢‘å°äºæ­¤å€¼çš„å•è¯ã€‚
- `workers(int, optional)` ï¼šè®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„çº¿ç¨‹æ•°ã€‚
- `sg({0,1}, optional)` ï¼šæ¨¡å‹çš„è®­ç»ƒç®—æ³•ï¼š1 ä»£è¡¨ skip-gramï¼›0 ä»£è¡¨ CBOWã€‚
- `hs({0,1}, optional)` ï¼š1 é‡‡ç”¨ Hierarchical Softmax è®­ç»ƒæ¨¡å‹ï¼›0 é‡‡ç”¨è´Ÿé‡‡æ ·ã€‚
- `epochs` ï¼šæ¨¡å‹è¿­ä»£çš„æ¬¡æ•°ã€‚
- `seed(int, optional)` ï¼šéšæœºæ•°å‘ç”Ÿå™¨ç§å­ã€‚

```python
text = [
    "å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒå¤„ç†é¢†åŸŸè·å¾—äº†æå¤§æˆåŠŸï¼Œå…¶ç»“åˆç‰¹å¾æå–å’Œç›®æ ‡è®­ç»ƒä¸ºä¸€ä½“çš„æ¨¡å‹èƒ½å¤Ÿæœ€å¥½çš„åˆ©ç”¨å·²æœ‰çš„ä¿¡æ¯å¯¹ç»“æœè¿›è¡Œåé¦ˆè®­ç»ƒã€‚",
    "å¯¹äºæ–‡æœ¬è¯†åˆ«çš„å·ç§¯ç¥ç»ç½‘ç»œæ¥è¯´ï¼ŒåŒæ ·ä¹Ÿæ˜¯å……åˆ†åˆ©ç”¨ç‰¹å¾æå–æ—¶æå–çš„æ–‡æœ¬ç‰¹å¾æ¥è®¡ç®—æ–‡æœ¬ç‰¹å¾æƒå€¼å¤§å°çš„ï¼Œå½’ä¸€åŒ–å¤„ç†éœ€è¦å¤„ç†çš„æ•°æ®ã€‚",
    "è¿™æ ·ä½¿å¾—åŸæ¥çš„æ–‡æœ¬ä¿¡æ¯æŠ½è±¡æˆä¸€ä¸ªå‘é‡åŒ–çš„æ ·æœ¬é›†ï¼Œä¹‹åå°†æ ·æœ¬é›†å’Œè®­ç»ƒå¥½çš„æ¨¡æ¿è¾“å…¥å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ã€‚",
    "æœ¬èŠ‚å°†åœ¨ä¸Šä¸€èŠ‚çš„åŸºç¡€ä¸Šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå®ç°æ–‡æœ¬åˆ†ç±»çš„é—®é¢˜ï¼Œè¿™é‡Œå°†é‡‡ç”¨ä¸¤ç§ä¸»è¦åŸºäºå­—ç¬¦çš„å’ŒåŸºäºword embeddingå½¢å¼çš„è¯å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†æ–¹æ³•ã€‚",
    "å®é™…ä¸Šæ— è®ºæ˜¯åŸºäºå­—ç¬¦çš„è¿˜æ˜¯åŸºäºword embeddingå½¢å¼çš„å¤„ç†æ–¹å¼éƒ½æ˜¯å¯ä»¥ç›¸äº’è½¬æ¢çš„ï¼Œè¿™é‡Œåªä»‹ç»ä½¿ç”¨åŸºæœ¬çš„ä½¿ç”¨æ¨¡å‹å’Œæ–¹æ³•ï¼Œæ›´å¤šçš„åº”ç”¨è¿˜éœ€è¦è¯»è€…è‡ªè¡ŒæŒ–æ˜å’Œè®¾è®¡ã€‚"
]

import jieba

jieba_cut_list = []
for line in text:
    jieba_cut = jieba.lcut(line)
    jieba_cut_list.append(jieba_cut)
    print(jieba_cut)

from gensim.models import FastText
model = FastText(min_count = 5, vector_size = 300, window = 7, workers = 10, epochs = 50, seed = 17, sg = 1, hs = 1)
model.build_vocab(jieba_cut_list)
model.train(jieba_cut_list, total_examples = model.corpus_count, epochs = model.epochs)
model.save("./fasttext_model_jieba.model")

model = FastText.load("./fasttext_model_jieba.model")
embedding = model.wv["è®¾è®¡"]
print(embedding)
print(embedding.shape)
```

### é’ˆå¯¹æ–‡æœ¬çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹â€”â€”å­—ç¬¦å·ç§¯

#### å­—ç¬¦ï¼ˆéå•è¯ï¼‰æ–‡æœ¬çš„å¤„ç†

ä»»ä½•ä¸€ä¸ªè‹±æ–‡å•è¯éƒ½æ˜¯ç”±å­—æ¯ç»„æˆçš„ï¼Œå› æ­¤å¯ä»¥ç®€å•åœ°å°†è‹±æ–‡å•è¯æ‹†åˆ†æˆå­—æ¯çš„è¡¨ç¤ºå½¢å¼ï¼š

> hello -> ["h","e","l","l","o"]

é‡‡ç”¨ One-Hot çš„æ–¹å¼æ¥å¤„ç†ï¼š

![image-20240718144844422](../../assets/image-20240718144844422.png)

ä½†æ˜¯å¯¹äºä¸åŒé•¿åº¦çš„å•è¯ï¼Œä¼šç”Ÿæˆä¸åŒçš„çŸ©é˜µï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œé•¿çš„æˆªçŸ­ï¼ŒçŸ­çš„è¡¥é•¿ã€‚

æœ€åæ„å»ºæ•°æ®é›†ã€‚

```python
import re
import csv
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

stoplist = stopwords.words('english')
def text_clear(text):
    text = text.lower()
    text = re.sub(r'[^a-z]', ' ', text)
    text = re.sub(r' +', ' ', text)
    text = text.strip()
    text = text.split(' ')
    text = [word for word in text if word not in stoplist]
    text = [PorterStemmer().stem(word) for word in text]
    text.append("eos")
    text = ["bos"] + text
    return text

def text_clearTitle(text):
    text = text.lower()
    text = re.sub(r'[^a-z]', ' ', text)
    text = re.sub(r' +', ' ', text)
    text = text.strip()
    text = text + " eos"
    return text

def get_label_one_hot(list):
    values = np.array(list)
    n_values = np.max(values) + 1
    return np.eye(n_values)[values]

def get_one_hot(list, alphabet_title = None):
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz "
    else:
        alphabet_title = alphabet_title
    values = np.array(list)
    n_values = len(alphabet_title) + 1
    return np.eye(n_values)[values]

def get_char_list(string, alphabet_title = None):
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz "
    else:
        alphabet_title = alphabet_title
    char_list = []
    for char in string:
        num = alphabet_title.index(char)
        char_list.append(num)
    return char_list

def get_string_matrix(string):
    char_list = get_char_list(string)
    string_matrix = get_one_hot(char_list)
    return string_matrix

def get_handle_string_matrix(string, n = 64):
    string_length = len(string)
    if string_length > n:
        string = string[:n]
        string_matrix = get_string_matrix(string)
        return string_matrix
    else:
        string_matrix = get_string_matrix(string)
        handle_length = n - string_length
        pad_matrix = np.zeros([handle_length, 28])
        string_matrix = np.concatenate([string_matrix, pad_matrix], axis = 0)
        return string_matrix
    
def get_dataset():
    agnews_label = []
    agnews_title = []
    agnews_train = csv.reader(open('train.csv', 'r'))
    for line in agnews_train:
        agnews_label.append(np.int32(line[0]))
        agnews_title.append(text_clearTitle(line[1]))
    train_dataset = []
    for title in agnews_title:
        string_matrix = get_handle_string_matrix(title)
        train_dataset.append(string_matrix)
    train_dataset = np.array(train_dataset)
    label_dataset = get_label_one_hot(agnews_label)
    return train_dataset, label_dataset

if __name__ == '__main__':
    train_dataset, label_dataset = get_dataset()
    print(train_dataset.shape)
    print(label_dataset.shape)
```

éšåä½¿ç”¨ä¸€ç»´å·ç§¯å¯¹å…¶è¿›è¡Œæ¨¡å‹å­¦ä¹ å³å¯ã€‚

### é’ˆå¯¹æ–‡æœ¬çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹â€”â€”è¯å·ç§¯

ä¸åŒäºå­—ç¬¦å·ç§¯ï¼Œè¯å·ç§¯å°†æ•´ä¸ªå•è¯ä½œä¸ºå•ä½è¿›è¡Œ One-Hot æ“ä½œï¼Œä½†æ˜¯å¸¸ç”¨çš„è‹±æ–‡å•è¯æˆ–è€…ä¸­æ–‡è¯è¯­ä¸€èˆ¬åœ¨ 5000 å·¦å³ï¼Œå»ºç«‹ä¸€ä¸ªç¨€ç–åºå¤§çš„ One-Hot çŸ©é˜µæ˜¯ä¸€ä¸ªä¸åˆ‡å®é™…çš„æƒ³æ³•ã€‚

ä½œä¸ºè§£å†³åŠæ³•ï¼Œä½¿ç”¨ Word2Vec çš„ Word Embeddingï¼Œé€šè¿‡å­¦ä¹ å°†å­—åº“ä¸­çš„è¯è½¬æ¢æˆç»´åº¦ä¸€å®šçš„å‘é‡ï¼Œä½œä¸ºå·ç§¯ç¥ç»ç½‘ç»œçš„è®¡ç®—ä¾æ®ã€‚

#### åˆ†è¯æ¨¡å‹çš„å¤„ç†

ä¸åŒäºå­—ç¬¦å·ç§¯ï¼Œæˆ‘ä»¬éœ€è¦è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å•è¯çš„åˆ—è¡¨ï¼Œæ–¹ä¾¿ Word2Vec å¤„ç†ã€‚

```python
def text_clearTitle_word2vec(text):
    text = text.lower()
    text = re.sub(r'[^a-z]', ' ', text)
    text = re.sub(r' +', ' ', text)
    text = text.strip()
    text = text + " eos"
    text = text.split(" ")
    return text
```

#### åˆ†è¯æ¨¡å‹çš„è®­ç»ƒä¸è½½å…¥

å¯¹äº Word2Vec è¯å‘é‡æ¥è¯´ï¼Œç®€å•åœ°å°†å¾…è¡¥å…¨çš„çŸ©é˜µç”¨å…¨ 0 è¡¥å…¨æ˜¯ä¸åˆé€‚çš„ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯å°†å…¨ 0 çŸ©é˜µä¿®æ”¹ä¸ºä¸€ä¸ªéå¸¸å°çš„å¸¸æ•°çŸ©é˜µã€‚

```python
def get_word2vec_dataset(n = 12):
    agnews_label = []
    agnews_title = []
    agnews_train = csv.reader(open('train.csv', 'r'))
    for line in agnews_train:
        agnews_label.append(np.int32(line[0]))
        agnews_title.append(text_clearTitle_word2vec(line[1]))
    from gensim.models import word2vec
    model = word2vec.Word2Vec(agnews_title, vector_size = 64, min_count = 0, window = 5)
    train_dataset = []
    for line in agnews_title:
        length = len(line)
        if(length > n):
            line = line[:n]
            word2vec_matrix = model.wv[line]
            train_dataset.append(word2vec_matrix)
        else:
            word2vec_matrix = model.wv[line]
            pad_length = n - length
            pad_matrix = np.zeros([pad_length, 64]) + 1e-10
            word2vec_matrix = np.concatenate((word2vec_matrix, pad_matrix), axis = 0)
            train_dataset.append(word2vec_matrix)
    train_dataset = np.expand_dims(train_dataset, 3) # ä¸ºä¸‹ä¸€æ­¥ä½¿ç”¨äºŒç»´å·ç§¯åšå‡†å¤‡
    label_dataset = get_label_one_hot(agnews_label)
    return train_dataset, label_dataset
```

æœ€åä½¿ç”¨äºŒç»´å·ç§¯å¯¹å…¶è¿›è¡Œæ¨¡å‹è®­ç»ƒå­¦ä¹ å³å¯ã€‚

```python
import torch
import einops.layers.torch as elt

def word2vec_CNN(input_dim = 28):
    model = torch.nn.Sequential(

        elt.Rearrange("b l d 1 -> b 1 l d"),
        #ç¬¬ä¸€å±‚å·ç§¯
        torch.nn.Conv2d(1,3,kernel_size=3),
        torch.nn.ReLU(),
        torch.nn.BatchNorm2d(num_features=3),

        #ç¬¬äºŒå±‚å·ç§¯
        torch.nn.Conv2d(3, 5, kernel_size=3),
        torch.nn.ReLU(),
        torch.nn.BatchNorm2d(num_features=5),

        #flatten
        torch.nn.Flatten(),  #[batch_size,64 * 28]
        torch.nn.Linear(2400,64),
        torch.nn.ReLU(),

        torch.nn.Linear(64,5),
        torch.nn.Softmax()
    )

    return model
#æ¨¡å‹è®­ç»ƒ
import get_data_84 as get_data
from sklearn.model_selection import train_test_split

train_dataset,label_dataset = get_data.get_word2vec_dataset()
X_train,X_test, y_train, y_test = train_test_split(train_dataset,label_dataset,test_size=0.1, random_state=828)  #å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

#è·å–device
device = "cuda" if torch.cuda.is_available() else "cpu"
model = word2vec_CNN().to(device)

# å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°
def cross_entropy(pred, label):
    res = -torch.sum(label * torch.log(pred)) / label.shape[0]
    return torch.mean(res)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

batch_size = 128
train_num = len(X_test)//128
for epoch in range(99):
    train_loss = 0.
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size

        x_batch = torch.tensor(X_train[start:end]).type(torch.float32).to(device)
        y_batch = torch.tensor(y_train[start:end]).type(torch.float32).to(device)

        pred = model(x_batch)
        loss = cross_entropy(pred, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()  # è®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„æŸå¤±å€¼

    # è®¡ç®—å¹¶æ‰“å°æŸå¤±å€¼
    train_loss /= train_num
    accuracy = (pred.argmax(1) == y_batch.argmax(1)).type(torch.float32).sum().item() / batch_size
    print("epochï¼š",epoch,"train_loss:", round(train_loss,2),"accuracy:",round(accuracy,2))
```

### æ±‰å­—çš„æ–‡æœ¬å¤„ç†

#### æ‹¼éŸ³åº“åŒ…

```python
from pypinyin import pinyin, lazy_pinyin, Style
value = lazy_pinyin('ä½ å¥½')
print(value)
```

![image-20240718155936841](../../assets/image-20240718155936841.png)

#### è¯å‘é‡

è¾ƒä¸ºå¸¸ç”¨ï¼Œä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œé™¤å»åœç”¨è¯å’Œå‰¯è¯ä¹‹ååˆ¶ä½œè¯å‘é‡ã€‚

```python
text = "åœ¨ä¸Šé¢çš„ç« èŠ‚ä¸­ï¼Œä½œè€…é€šè¿‡ä¸åŒçš„å·ç§¯ï¼ˆä¸€ç»´å·ç§¯å’ŒäºŒç»´å·ç§¯ï¼‰å®ç°äº†æ–‡æœ¬çš„åˆ†ç±»ï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨GensimæŒæ¡äº†å¯¹æ–‡æœ¬è¿›è¡Œè¯å‘é‡è½¬æ¢çš„æ–¹æ³•ã€‚è¯å‘é‡Word Embeddingæ˜¯ç›®å‰æœ€å¸¸ç”¨çš„å°†æ–‡æœ¬è½¬æˆå‘é‡çš„æ–¹æ³•ï¼Œæ¯”è¾ƒé€‚åˆè¾ƒä¸ºå¤æ‚è¯è¢‹ä¸­è¯ç»„è¾ƒå¤šçš„æƒ…å†µã€‚ä½¿ç”¨one-hotæ–¹æ³•å¯¹å­—ç¬¦è¿›è¡Œè¡¨ç¤ºæ˜¯ä¸€ç§éå¸¸ç®€å•çš„æ–¹æ³•ï¼Œä½†æ˜¯ç”±äºå…¶ä½¿ç”¨å—é™è¾ƒå¤§ï¼Œäº§ç”Ÿçš„çŸ©é˜µè¾ƒä¸ºç¨€ç–ï¼Œå› æ­¤åœ¨å®ç”¨æ€§ä¸Šå¹¶ä¸æ˜¯å¾ˆå¼ºï¼Œä½œè€…åœ¨è¿™é‡Œç»Ÿä¸€æ¨èä½¿ç”¨Word Embeddingçš„æ–¹å¼å¯¹è¯è¿›è¡Œå¤„ç†ã€‚å¯èƒ½æœ‰è¯»è€…ä¼šäº§ç”Ÿç–‘é—®ï¼Œå¦‚æœä½¿ç”¨Word2Vecçš„å½¢å¼æ¥è®¡ç®—å­—ç¬¦çš„â€œå­—å‘é‡â€æ˜¯å¦å¯è¡Œã€‚é‚£ä¹ˆä½œè€…çš„ç­”æ¡ˆæ˜¯å®Œå…¨å¯ä»¥ï¼Œå¹¶ä¸”å‡†ç¡®åº¦ç›¸å¯¹äºå•çº¯é‡‡ç”¨one-hotå½¢å¼çš„çŸ©é˜µè¡¨ç¤ºï¼Œéƒ½èƒ½æœ‰æ›´å¥½çš„è¡¨ç°å’Œå‡†ç¡®åº¦ã€‚"

import jieba
import re
from gensim.models import word2vec

text = re.sub(r"[a-zA-Z0-9-ï¼Œã€‚â€œâ€ï¼ˆï¼‰]", " " , text)
text = re.sub(r" +", " ", text)
text = re.sub(" ", "", text)
print(text)
text_list = jieba.lcut_for_search(text)
model = word2vec.Word2Vec([text_list], vector_size = 50, min_count = 1, window = 3)
print(model.wv["ç« èŠ‚"])
```

## å¾ªç¯ç¥ç»ç½‘ç»œ

### GRU

å¾ªç¯ç¥ç»ç½‘ç»œç”¨æ¥å¤„ç†åºåˆ—æ•°æ®ã€‚ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ˜¯ä»è¾“å…¥å±‚åˆ°éšè—å±‚å†åˆ°è¾“å‡ºå±‚ï¼Œå±‚ä¸å±‚ä¹‹é—´æ˜¯å…¨é“¾æ¥çš„ï¼Œæ¯å±‚ä¹‹é—´çš„èŠ‚ç‚¹æ˜¯æ— è¿æ¥çš„ï¼Œåœ¨å¤„ç†åºåˆ—è¾“å‡ºä¸å‰é¢çš„è¾“å‡ºæœ‰å…³çš„é—®é¢˜æ— èƒ½ä¸ºåŠ›ï¼ˆä¾‹å¦‚æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹å¥å­çš„ä¸‹ä¸€ä¸ªå•è¯æ˜¯ä»€ä¹ˆï¼‰ï¼›å¾ªç¯ç¥ç»ç½‘ç»œä¼šå¯¹å‰é¢çš„ä¿¡æ¯è¿›è¡Œè®°å¿†å…µè¥ç”¨äºå½“å‰è¾“å‡ºçš„è®¡ç®—ä¸­ï¼Œå³éšè—å±‚ä¹‹é—´çš„èŠ‚ç‚¹ä¸å†æ˜¯æ— è¿æ¥çš„ï¼Œè€Œæ˜¯æœ‰è¿æ¥çš„ï¼Œå¹¶ä¸”éšè—å±‚çš„è¾“å…¥ä¸ä»…åŒ…æ‹¬è¾“å…¥å±‚çš„è¾“å‡ºï¼Œè¿˜åŒ…æ‹¬ä¸Šä¸€æ—¶åˆ»éšè—å±‚çš„è¾“å‡ºã€‚

![img](https://ai-studio-static-online.cdn.bcebos.com/36f6c180c83e48a2ad69e9d16026934709a2a056a1714127b2e8e52705bb80b4)

GRU æ˜¯å¾ªç¯ç¥ç»ç½‘ç»œçš„ä¸€ç§ï¼Œä¸ºäº†è§£å†³é•¿æœŸè®°å¿†å’Œåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦ç­‰é—®é¢˜ï¼Œæ˜¯ä¸€ç§ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œæ›´æ“…é•¿å¤„ç†åºåˆ—å˜åŒ–çš„æ•°æ®ï¼ˆä¾‹å¦‚æŸä¸ªå•è¯çš„æ„æ€ä¼šå› ä¸ºä¸Šæ–‡æåˆ°çš„å†…å®¹ä¸åŒè€Œæœ‰ä¸åŒçš„å«ä¹‰ï¼‰

åœ¨ GRU ä¸­æœ‰ä¸€ä¸ªå½“å‰çš„è¾“å…¥ $x^t$ ï¼Œå’Œä¸Šä¸€ä¸ªèŠ‚ç‚¹ä¼ é€’ä¸‹æ¥çš„éšè—çŠ¶æ€ï¼ˆHidden Stateï¼‰$h^{t-1}$ ï¼ˆåŒ…å«ä¹‹å‰èŠ‚ç‚¹çš„ç›¸å…³ä¿¡æ¯ï¼‰ï¼Œç»“åˆ $x^t$ å’Œ $h^{t-1}$ï¼ŒGRUä¼šå¾—åˆ°å½“å‰éšè—èŠ‚ç‚¹çš„è¾“å‡º $y^t$ å’Œä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„éšè—çŠ¶æ€ $h^t$ã€‚

<img src="../../assets/image-20240718163949208.png" alt="image-20240718163949208" style="zoom:50%;" />

GRU åˆ©ç”¨é—¨å…ˆé€šè¿‡ä¸Šä¸€ä¸ªä¼ è¾“ä¸‹æ¥çš„çŠ¶æ€ $h^{t-1}$ å’Œå½“å‰èŠ‚ç‚¹çš„è¾“å…¥ $x^t$ æ¥è·å–ä¸¤ä¸ªé—¨æ§çŠ¶æ€ï¼Œ$r$ ç”¨äºæ§åˆ¶é‡ç½®çš„é—¨æ§ï¼ˆReset Gateï¼‰ï¼Œ$z$ ç”¨äºæ§åˆ¶æ›´æ–°çš„é—¨æ§ï¼ˆUpdate Gateï¼‰ã€‚$\sigma$ ä¸º Sigmoid å‡½æ•°ï¼Œé€šè¿‡è¿™ä¸ªå‡½æ•°å¯ä»¥å°†æ•°æ®å˜æ¢ä¸º 0-1 èŒƒå›´å†…çš„æ•°å€¼ï¼Œä»è€Œæ¥å……å½“é—¨æ§ä¿¡å·ã€‚

<img src="../../assets/image-20240718165853423.png" alt="image-20240718165853423" style="zoom:50%;" />

å¾—åˆ°é—¨æ§ä¿¡å·ä¹‹åï¼Œé¦–å…ˆä½¿ç”¨é‡ç½®é—¨æ§æ¥å¾—åˆ°é‡ç½®ä¹‹åçš„æ•°æ® $h^{(t-1)'}=h^{t-1}\times r$ ï¼Œå†å°† $h^{(t-1)'}$ ä¸è¾“å…¥ $x^t$ è¿›è¡Œæ‹¼æ¥ï¼Œé€šè¿‡ä¸€ä¸ª Tanh æ¿€æ´»å‡½æ•°æ¥å°†æ•°æ®æ‰€æ”¾åˆ° -1-1 çš„èŒƒå›´å†…ï¼Œå¾—åˆ° $h'$ ï¼Œå­˜å‚¨å½“å‰æ—¶åˆ»çš„çŠ¶æ€ã€‚

<img src="../../assets/image-20240718170344993.png" alt="image-20240718170344993" style="zoom:50%;" />

æœ€åä¾¿æ˜¯ GRU æœ€å…³é”®çš„æ­¥éª¤â€”â€”â€œæ›´æ–°è®°å¿†â€é˜¶æ®µï¼ŒåŒæ—¶è¿›è¡Œé—å¿˜å’Œè®°å¿†ä¸¤ä¸ªæ­¥éª¤ã€‚

<img src="https://i-blog.csdnimg.cn/blog_migrate/d7b12bd21b6447bf45830c9717e3e3f8.png" alt="img" style="zoom:50%;" />

ä½¿ç”¨å…ˆå‰å¾—åˆ°çš„æ›´æ–°é—¨æ§ zï¼Œä»è€Œèƒ½å¤Ÿè·å¾—æ–°çš„æ›´æ–°ï¼š$h^t=zh^{t-1}+(1-z)h'$

- $zh^{t-1}$ï¼šè¡¨ç¤ºå¯¹åŸæœ¬çš„éšè—çŠ¶æ€é€‰æ‹©æ€§â€œé—å¿˜â€ã€‚è¿™é‡Œçš„ z å¯ä»¥æƒ³è±¡æˆé—å¿˜é—¨ï¼ˆForget Gateï¼‰ï¼Œå¿˜è®° $h^{t-1}$ ç»´åº¦ä¸­ä¸€äº›ä¸é‡è¦çš„ä¿¡æ¯ã€‚
- $(1-z)h'$ ï¼šè¡¨ç¤ºå¯¹åŒ…å«å½“å‰èŠ‚ç‚¹ä¿¡æ¯çš„ $h'$ è¿›è¡Œé€‰æ‹©æ€§â€œè®°å¿†â€ã€‚å¯çœ‹ä½œæ˜¯å¯¹ $h'$ ç»´åº¦ä¸­çš„ä¸€äº›ä¿¡æ¯è¿›è¡Œé€‰æ‹©ã€‚

### åŒå‘ GRU

åŒå‘ GRU å°†ç›¸åŒçš„ä¿¡æ¯ä»¥ä¸åŒçš„æ–¹å¼å‘ˆç°ç»™å¾ªç¯ç½‘ç»œï¼Œæ˜¯ä¸€ç§å¸¸è§çš„ GRU å˜ä½“ï¼Œå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚

å•å‘ GRU ç‰¹åˆ«ä¾èµ–äºé¡ºåºæˆ–æ—¶é—´ï¼ŒæŒ‰é¡ºåºå¤„ç†è¾“å…¥åºåˆ—çš„æ—¶é—´æ­¥ï¼Œè€Œæ‰“ä¹±æ—¶é—´æ­¥æˆ–åè½¬æ—¶é—´æ­¥ä¼šå®Œå…¨æ”¹å˜ GRU ä»åºåˆ—ä¸­æå–çš„è¡¨ç¤ºã€‚å¦‚æœé¡ºåºå¯¹é—®é¢˜å¾ˆé‡è¦ï¼ˆæ¯”å¦‚å®¤æ¸©é¢„æµ‹ç­‰é—®é¢˜ï¼‰ï¼ŒGRU çš„è¡¨ç°å°±ä¼šå¾ˆå¥½ã€‚

åŒå‘ GRU ä¸­çš„æ¯ä¸€ä¸ª GRU åˆ†åˆ«æ²¿ä¸€ä¸ªæ–¹å‘å¯¹è¾“å…¥åºåˆ—è¿›è¡Œå¤„ç†ï¼ˆæ—¶é—´æ­£åºå’Œæ—¶é—´é€†åºï¼‰ï¼Œç„¶åå°†å®ƒä»¬çš„è¡¨ç¤ºåˆå¹¶åœ¨ä¸€èµ·ã€‚

![26. ç°¡ä»‹é›™å‘éè¿´ç¥ç¶“ç¶²è·¯(BiRNN) - Programming with Data - Medium](data:image/png;base64,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)

å¯¹äºæ–‡æœ¬åˆ†ç±»ç­‰é—®é¢˜ï¼Œä¸€ä¸ªå•è¯å¯¹ç†è§£å¥å­çš„é‡è¦æ€§é€šå¸¸å¹¶ä¸å–å†³äºå®ƒåœ¨å¥å­ä¸­çš„ä½ç½®ï¼Œå³ç”¨æ­£åºåºåˆ—å’Œé€†åºåºåˆ—ï¼Œæˆ–è€…éšæœºæ‰“æ–­å…¶å‡ºç°çš„ä½ç½®ï¼Œå°†æ–°çš„æ•°æ®ä½œä¸ºæ ·æœ¬è¾“å…¥ç»™ GRU è¿›è¡Œé‡æ–°è®­ç»ƒå¹¶è¯„ä¼°ï¼Œæ€§èƒ½å‡ ä¹ç›¸åŒã€‚

åŒå‘å¾ªç¯å±‚è¿˜æœ‰ä¸€ä¸ªå¥½å¤„åœ¨äºå¦‚æœä¸€ç§æ•°æ®è¡¨ç¤ºä¸åŒä½†æœ‰ç”¨ï¼Œé‚£ä¹ˆæ€»æ˜¯å€¼å¾—åŠ ä»¥åˆ©ç”¨ï¼Œè¿™æä¾›äº†æŸ¥çœ‹æ•°æ®çš„å…¨æ–°è§’åº¦ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨æŸä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚